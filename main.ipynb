{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00c9334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca504cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = r\"D:\\Final EEG Graph Project\\EEG Images\"  # your dataset root folder\n",
    "\n",
    "# Data transforms (rescale to image size and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2212, 1408)),  # Ensure all images are the same size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831db60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06cb5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten helper\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# CNN Model using Sequential\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),  # -> (16, 1104, 702)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                         # -> (16, 552, 351)\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3),           # -> (32, 550, 349)\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2),                         # -> (32, 275, 174)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3),           # -> (64, 273, 172)\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2),                         # -> (64, 136, 86)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3),          # -> (128, 136, 86)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                         # -> (128, 68, 43)\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3),          # -> (256, 66, 41)\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2),                         # -> (256, 33, 20)\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3),          # -> (512, 66, 41)\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.MaxPool2d(2),                         # -> (512, 33, 20)\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "\n",
    "            Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1998f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 68.6141\n",
      "✅ Model saved: models/model_epoch_1.pth and models/full_model_epoch_1.pt\n",
      "Epoch 2/100, Loss: 63.6055\n",
      "✅ Model saved: models/model_epoch_2.pth and models/full_model_epoch_2.pt\n",
      "Epoch 3/100, Loss: 52.5961\n",
      "✅ Model saved: models/model_epoch_3.pth and models/full_model_epoch_3.pt\n",
      "Epoch 4/100, Loss: 47.1886\n",
      "✅ Model saved: models/model_epoch_4.pth and models/full_model_epoch_4.pt\n",
      "Epoch 5/100, Loss: 47.9271\n",
      "✅ Model saved: models/model_epoch_5.pth and models/full_model_epoch_5.pt\n",
      "Epoch 6/100, Loss: 44.3620\n",
      "✅ Model saved: models/model_epoch_6.pth and models/full_model_epoch_6.pt\n",
      "Epoch 7/100, Loss: 43.9893\n",
      "✅ Model saved: models/model_epoch_7.pth and models/full_model_epoch_7.pt\n",
      "Epoch 8/100, Loss: 42.6662\n",
      "✅ Model saved: models/model_epoch_8.pth and models/full_model_epoch_8.pt\n",
      "Epoch 9/100, Loss: 41.8517\n",
      "✅ Model saved: models/model_epoch_9.pth and models/full_model_epoch_9.pt\n",
      "Epoch 10/100, Loss: 42.9485\n",
      "✅ Model saved: models/model_epoch_10.pth and models/full_model_epoch_10.pt\n",
      "Epoch 11/100, Loss: 41.3034\n",
      "✅ Model saved: models/model_epoch_11.pth and models/full_model_epoch_11.pt\n",
      "Epoch 12/100, Loss: 40.5917\n",
      "✅ Model saved: models/model_epoch_12.pth and models/full_model_epoch_12.pt\n",
      "Epoch 13/100, Loss: 38.2761\n",
      "✅ Model saved: models/model_epoch_13.pth and models/full_model_epoch_13.pt\n",
      "Epoch 14/100, Loss: 38.8322\n",
      "✅ Model saved: models/model_epoch_14.pth and models/full_model_epoch_14.pt\n",
      "Epoch 15/100, Loss: 37.2747\n",
      "✅ Model saved: models/model_epoch_15.pth and models/full_model_epoch_15.pt\n",
      "Epoch 16/100, Loss: 36.8412\n",
      "✅ Model saved: models/model_epoch_16.pth and models/full_model_epoch_16.pt\n",
      "Epoch 17/100, Loss: 36.5232\n",
      "✅ Model saved: models/model_epoch_17.pth and models/full_model_epoch_17.pt\n",
      "Epoch 18/100, Loss: 35.7426\n",
      "✅ Model saved: models/model_epoch_18.pth and models/full_model_epoch_18.pt\n",
      "Epoch 19/100, Loss: 34.2000\n",
      "✅ Model saved: models/model_epoch_19.pth and models/full_model_epoch_19.pt\n",
      "Epoch 20/100, Loss: 33.1241\n",
      "✅ Model saved: models/model_epoch_20.pth and models/full_model_epoch_20.pt\n",
      "Epoch 21/100, Loss: 33.7003\n",
      "✅ Model saved: models/model_epoch_21.pth and models/full_model_epoch_21.pt\n",
      "Epoch 22/100, Loss: 31.2540\n",
      "✅ Model saved: models/model_epoch_22.pth and models/full_model_epoch_22.pt\n",
      "Epoch 23/100, Loss: 31.5327\n",
      "✅ Model saved: models/model_epoch_23.pth and models/full_model_epoch_23.pt\n",
      "Epoch 24/100, Loss: 28.9933\n",
      "✅ Model saved: models/model_epoch_24.pth and models/full_model_epoch_24.pt\n",
      "Epoch 25/100, Loss: 29.6550\n",
      "✅ Model saved: models/model_epoch_25.pth and models/full_model_epoch_25.pt\n",
      "Epoch 26/100, Loss: 28.9704\n",
      "✅ Model saved: models/model_epoch_26.pth and models/full_model_epoch_26.pt\n",
      "Epoch 27/100, Loss: 26.5026\n",
      "✅ Model saved: models/model_epoch_27.pth and models/full_model_epoch_27.pt\n",
      "Epoch 28/100, Loss: 24.6607\n",
      "✅ Model saved: models/model_epoch_28.pth and models/full_model_epoch_28.pt\n",
      "Epoch 29/100, Loss: 22.5348\n",
      "✅ Model saved: models/model_epoch_29.pth and models/full_model_epoch_29.pt\n",
      "Epoch 30/100, Loss: 19.1044\n",
      "✅ Model saved: models/model_epoch_30.pth and models/full_model_epoch_30.pt\n",
      "Epoch 31/100, Loss: 20.0957\n",
      "✅ Model saved: models/model_epoch_31.pth and models/full_model_epoch_31.pt\n",
      "Epoch 32/100, Loss: 16.5274\n",
      "✅ Model saved: models/model_epoch_32.pth and models/full_model_epoch_32.pt\n",
      "Epoch 33/100, Loss: 18.3101\n",
      "✅ Model saved: models/model_epoch_33.pth and models/full_model_epoch_33.pt\n",
      "Epoch 34/100, Loss: 16.6884\n",
      "✅ Model saved: models/model_epoch_34.pth and models/full_model_epoch_34.pt\n",
      "Epoch 35/100, Loss: 10.6630\n",
      "✅ Model saved: models/model_epoch_35.pth and models/full_model_epoch_35.pt\n",
      "Epoch 36/100, Loss: 7.3349\n",
      "✅ Model saved: models/model_epoch_36.pth and models/full_model_epoch_36.pt\n",
      "Epoch 37/100, Loss: 7.9213\n",
      "✅ Model saved: models/model_epoch_37.pth and models/full_model_epoch_37.pt\n",
      "Epoch 38/100, Loss: 4.8972\n",
      "✅ Model saved: models/model_epoch_38.pth and models/full_model_epoch_38.pt\n",
      "Epoch 39/100, Loss: 10.3259\n",
      "✅ Model saved: models/model_epoch_39.pth and models/full_model_epoch_39.pt\n",
      "Epoch 40/100, Loss: 4.4589\n",
      "✅ Model saved: models/model_epoch_40.pth and models/full_model_epoch_40.pt\n",
      "Epoch 41/100, Loss: 3.9881\n",
      "✅ Model saved: models/model_epoch_41.pth and models/full_model_epoch_41.pt\n",
      "Epoch 42/100, Loss: 2.5819\n",
      "✅ Model saved: models/model_epoch_42.pth and models/full_model_epoch_42.pt\n",
      "Epoch 43/100, Loss: 2.9130\n",
      "✅ Model saved: models/model_epoch_43.pth and models/full_model_epoch_43.pt\n",
      "Epoch 44/100, Loss: 2.0854\n",
      "✅ Model saved: models/model_epoch_44.pth and models/full_model_epoch_44.pt\n",
      "Epoch 45/100, Loss: 1.1524\n",
      "✅ Model saved: models/model_epoch_45.pth and models/full_model_epoch_45.pt\n",
      "Epoch 46/100, Loss: 1.6016\n",
      "✅ Model saved: models/model_epoch_46.pth and models/full_model_epoch_46.pt\n",
      "Epoch 47/100, Loss: 1.8409\n",
      "✅ Model saved: models/model_epoch_47.pth and models/full_model_epoch_47.pt\n",
      "Epoch 48/100, Loss: 8.4203\n",
      "✅ Model saved: models/model_epoch_48.pth and models/full_model_epoch_48.pt\n",
      "Epoch 49/100, Loss: 2.3921\n",
      "✅ Model saved: models/model_epoch_49.pth and models/full_model_epoch_49.pt\n",
      "Epoch 50/100, Loss: 1.4939\n",
      "✅ Model saved: models/model_epoch_50.pth and models/full_model_epoch_50.pt\n",
      "Epoch 51/100, Loss: 1.4838\n",
      "✅ Model saved: models/model_epoch_51.pth and models/full_model_epoch_51.pt\n",
      "Epoch 52/100, Loss: 1.0230\n",
      "✅ Model saved: models/model_epoch_52.pth and models/full_model_epoch_52.pt\n",
      "Epoch 53/100, Loss: 1.0944\n",
      "✅ Model saved: models/model_epoch_53.pth and models/full_model_epoch_53.pt\n",
      "Epoch 54/100, Loss: 1.1638\n",
      "✅ Model saved: models/model_epoch_54.pth and models/full_model_epoch_54.pt\n",
      "Epoch 55/100, Loss: 1.3323\n",
      "✅ Model saved: models/model_epoch_55.pth and models/full_model_epoch_55.pt\n",
      "Epoch 56/100, Loss: 1.1726\n",
      "✅ Model saved: models/model_epoch_56.pth and models/full_model_epoch_56.pt\n",
      "Epoch 57/100, Loss: 1.3659\n",
      "✅ Model saved: models/model_epoch_57.pth and models/full_model_epoch_57.pt\n",
      "Epoch 58/100, Loss: 0.9996\n",
      "✅ Model saved: models/model_epoch_58.pth and models/full_model_epoch_58.pt\n",
      "Epoch 59/100, Loss: 0.9416\n",
      "✅ Model saved: models/model_epoch_59.pth and models/full_model_epoch_59.pt\n",
      "Epoch 60/100, Loss: 0.8343\n",
      "✅ Model saved: models/model_epoch_60.pth and models/full_model_epoch_60.pt\n",
      "Epoch 61/100, Loss: 1.1998\n",
      "✅ Model saved: models/model_epoch_61.pth and models/full_model_epoch_61.pt\n",
      "Epoch 62/100, Loss: 1.6629\n",
      "✅ Model saved: models/model_epoch_62.pth and models/full_model_epoch_62.pt\n",
      "Epoch 63/100, Loss: 1.8297\n",
      "✅ Model saved: models/model_epoch_63.pth and models/full_model_epoch_63.pt\n",
      "Epoch 64/100, Loss: 1.0670\n",
      "✅ Model saved: models/model_epoch_64.pth and models/full_model_epoch_64.pt\n",
      "Epoch 65/100, Loss: 0.9019\n",
      "✅ Model saved: models/model_epoch_65.pth and models/full_model_epoch_65.pt\n",
      "Epoch 66/100, Loss: 0.8070\n",
      "✅ Model saved: models/model_epoch_66.pth and models/full_model_epoch_66.pt\n",
      "Epoch 67/100, Loss: 0.9687\n",
      "✅ Model saved: models/model_epoch_67.pth and models/full_model_epoch_67.pt\n",
      "Epoch 68/100, Loss: 0.9420\n",
      "✅ Model saved: models/model_epoch_68.pth and models/full_model_epoch_68.pt\n",
      "Epoch 69/100, Loss: 0.8624\n",
      "✅ Model saved: models/model_epoch_69.pth and models/full_model_epoch_69.pt\n",
      "Epoch 70/100, Loss: 0.8083\n",
      "✅ Model saved: models/model_epoch_70.pth and models/full_model_epoch_70.pt\n",
      "Epoch 71/100, Loss: 0.9209\n",
      "✅ Model saved: models/model_epoch_71.pth and models/full_model_epoch_71.pt\n",
      "Epoch 72/100, Loss: 0.7403\n",
      "✅ Model saved: models/model_epoch_72.pth and models/full_model_epoch_72.pt\n",
      "Epoch 73/100, Loss: 0.6770\n",
      "✅ Model saved: models/model_epoch_73.pth and models/full_model_epoch_73.pt\n",
      "Epoch 74/100, Loss: 0.7075\n",
      "✅ Model saved: models/model_epoch_74.pth and models/full_model_epoch_74.pt\n",
      "Epoch 75/100, Loss: 0.6696\n",
      "✅ Model saved: models/model_epoch_75.pth and models/full_model_epoch_75.pt\n",
      "Epoch 76/100, Loss: 0.7026\n",
      "✅ Model saved: models/model_epoch_76.pth and models/full_model_epoch_76.pt\n",
      "Epoch 77/100, Loss: 0.7104\n",
      "✅ Model saved: models/model_epoch_77.pth and models/full_model_epoch_77.pt\n",
      "Epoch 78/100, Loss: 0.6382\n",
      "✅ Model saved: models/model_epoch_78.pth and models/full_model_epoch_78.pt\n",
      "Epoch 79/100, Loss: 1.0181\n",
      "✅ Model saved: models/model_epoch_79.pth and models/full_model_epoch_79.pt\n",
      "Epoch 80/100, Loss: 1.1121\n",
      "✅ Model saved: models/model_epoch_80.pth and models/full_model_epoch_80.pt\n",
      "Epoch 81/100, Loss: 2.9908\n",
      "✅ Model saved: models/model_epoch_81.pth and models/full_model_epoch_81.pt\n",
      "Epoch 82/100, Loss: 17.1128\n",
      "✅ Model saved: models/model_epoch_82.pth and models/full_model_epoch_82.pt\n",
      "Epoch 83/100, Loss: 2.4776\n",
      "✅ Model saved: models/model_epoch_83.pth and models/full_model_epoch_83.pt\n",
      "Epoch 84/100, Loss: 1.1281\n",
      "✅ Model saved: models/model_epoch_84.pth and models/full_model_epoch_84.pt\n",
      "Epoch 85/100, Loss: 0.8750\n",
      "✅ Model saved: models/model_epoch_85.pth and models/full_model_epoch_85.pt\n",
      "Epoch 86/100, Loss: 1.3247\n",
      "✅ Model saved: models/model_epoch_86.pth and models/full_model_epoch_86.pt\n",
      "Epoch 87/100, Loss: 1.1586\n",
      "✅ Model saved: models/model_epoch_87.pth and models/full_model_epoch_87.pt\n",
      "Epoch 88/100, Loss: 0.7707\n",
      "✅ Model saved: models/model_epoch_88.pth and models/full_model_epoch_88.pt\n",
      "Epoch 89/100, Loss: 0.6665\n",
      "✅ Model saved: models/model_epoch_89.pth and models/full_model_epoch_89.pt\n",
      "Epoch 90/100, Loss: 0.6706\n",
      "✅ Model saved: models/model_epoch_90.pth and models/full_model_epoch_90.pt\n",
      "Epoch 91/100, Loss: 0.7004\n",
      "✅ Model saved: models/model_epoch_91.pth and models/full_model_epoch_91.pt\n",
      "Epoch 92/100, Loss: 0.7123\n",
      "✅ Model saved: models/model_epoch_92.pth and models/full_model_epoch_92.pt\n",
      "Epoch 93/100, Loss: 0.6734\n",
      "✅ Model saved: models/model_epoch_93.pth and models/full_model_epoch_93.pt\n",
      "Epoch 94/100, Loss: 0.6577\n",
      "✅ Model saved: models/model_epoch_94.pth and models/full_model_epoch_94.pt\n",
      "Epoch 95/100, Loss: 0.6888\n",
      "✅ Model saved: models/model_epoch_95.pth and models/full_model_epoch_95.pt\n",
      "Epoch 96/100, Loss: 0.6963\n",
      "✅ Model saved: models/model_epoch_96.pth and models/full_model_epoch_96.pt\n",
      "Epoch 97/100, Loss: 0.6100\n",
      "✅ Model saved: models/model_epoch_97.pth and models/full_model_epoch_97.pt\n",
      "Epoch 98/100, Loss: 0.7353\n",
      "✅ Model saved: models/model_epoch_98.pth and models/full_model_epoch_98.pt\n",
      "Epoch 99/100, Loss: 0.6802\n",
      "✅ Model saved: models/model_epoch_99.pth and models/full_model_epoch_99.pt\n",
      "Epoch 100/100, Loss: 0.7627\n",
      "✅ Model saved: models/model_epoch_100.pth and models/full_model_epoch_100.pt\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f\"D:/Final EEG Graph Project/models/model_state_dict/model_state_dict_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model, f\"D:/Final EEG Graph Project/models/full_model/full_model_epoch_{epoch+1}.pt\")\n",
    "    print(f\"✅ Model saved: models/model_epoch_{epoch+1}.pth and models/full_model_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4241abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyes\\AppData\\Local\\Temp\\ipykernel_17284\\664528689.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy = 49.24%\n",
      "Epoch 2: Validation Accuracy = 68.53%\n",
      "Epoch 3: Validation Accuracy = 72.08%\n",
      "Epoch 4: Validation Accuracy = 71.07%\n",
      "Epoch 5: Validation Accuracy = 77.16%\n",
      "Epoch 6: Validation Accuracy = 75.13%\n",
      "Epoch 7: Validation Accuracy = 78.68%\n",
      "Epoch 8: Validation Accuracy = 80.20%\n",
      "Epoch 9: Validation Accuracy = 79.19%\n",
      "Epoch 10: Validation Accuracy = 78.17%\n",
      "Epoch 11: Validation Accuracy = 80.20%\n",
      "Epoch 12: Validation Accuracy = 79.70%\n",
      "Epoch 13: Validation Accuracy = 80.71%\n",
      "Epoch 14: Validation Accuracy = 80.20%\n",
      "Epoch 15: Validation Accuracy = 78.17%\n",
      "Epoch 16: Validation Accuracy = 79.70%\n",
      "Epoch 17: Validation Accuracy = 80.71%\n",
      "Epoch 18: Validation Accuracy = 79.19%\n",
      "Epoch 19: Validation Accuracy = 83.76%\n",
      "Epoch 20: Validation Accuracy = 76.65%\n",
      "Epoch 21: Validation Accuracy = 80.71%\n",
      "Epoch 22: Validation Accuracy = 84.26%\n",
      "Epoch 23: Validation Accuracy = 81.22%\n",
      "Epoch 24: Validation Accuracy = 80.20%\n",
      "Epoch 25: Validation Accuracy = 77.16%\n",
      "Epoch 26: Validation Accuracy = 81.22%\n",
      "Epoch 27: Validation Accuracy = 81.73%\n",
      "Epoch 28: Validation Accuracy = 82.23%\n",
      "Epoch 29: Validation Accuracy = 82.74%\n",
      "Epoch 30: Validation Accuracy = 81.73%\n",
      "Epoch 31: Validation Accuracy = 81.22%\n",
      "Epoch 32: Validation Accuracy = 82.23%\n",
      "Epoch 33: Validation Accuracy = 81.73%\n",
      "Epoch 34: Validation Accuracy = 80.71%\n",
      "Epoch 35: Validation Accuracy = 80.71%\n",
      "Epoch 36: Validation Accuracy = 79.19%\n",
      "Epoch 37: Validation Accuracy = 81.22%\n",
      "Epoch 38: Validation Accuracy = 81.73%\n",
      "Epoch 39: Validation Accuracy = 80.71%\n",
      "Epoch 40: Validation Accuracy = 80.20%\n",
      "Epoch 41: Validation Accuracy = 81.73%\n",
      "Epoch 42: Validation Accuracy = 79.19%\n",
      "Epoch 43: Validation Accuracy = 79.19%\n",
      "Epoch 44: Validation Accuracy = 78.17%\n",
      "Epoch 45: Validation Accuracy = 79.19%\n",
      "Epoch 46: Validation Accuracy = 81.22%\n",
      "Epoch 47: Validation Accuracy = 79.70%\n",
      "Epoch 48: Validation Accuracy = 78.17%\n",
      "Epoch 49: Validation Accuracy = 81.22%\n",
      "Epoch 50: Validation Accuracy = 80.20%\n",
      "Epoch 51: Validation Accuracy = 80.20%\n",
      "Epoch 52: Validation Accuracy = 80.71%\n",
      "Epoch 53: Validation Accuracy = 80.20%\n",
      "Epoch 54: Validation Accuracy = 81.22%\n",
      "Epoch 55: Validation Accuracy = 80.20%\n",
      "Epoch 56: Validation Accuracy = 80.20%\n",
      "Epoch 57: Validation Accuracy = 78.17%\n",
      "Epoch 58: Validation Accuracy = 80.20%\n",
      "Epoch 59: Validation Accuracy = 80.20%\n",
      "Epoch 60: Validation Accuracy = 80.20%\n",
      "Epoch 61: Validation Accuracy = 79.70%\n",
      "Epoch 62: Validation Accuracy = 79.70%\n",
      "Epoch 63: Validation Accuracy = 77.16%\n",
      "Epoch 64: Validation Accuracy = 78.17%\n",
      "Epoch 65: Validation Accuracy = 79.19%\n",
      "Epoch 66: Validation Accuracy = 80.20%\n",
      "Epoch 67: Validation Accuracy = 80.71%\n",
      "Epoch 68: Validation Accuracy = 80.20%\n",
      "Epoch 69: Validation Accuracy = 80.71%\n",
      "Epoch 70: Validation Accuracy = 76.65%\n",
      "Epoch 71: Validation Accuracy = 78.68%\n",
      "Epoch 72: Validation Accuracy = 78.68%\n",
      "Epoch 73: Validation Accuracy = 78.17%\n",
      "Epoch 74: Validation Accuracy = 78.17%\n",
      "Epoch 75: Validation Accuracy = 76.65%\n",
      "Epoch 76: Validation Accuracy = 78.68%\n",
      "Epoch 77: Validation Accuracy = 79.19%\n",
      "Epoch 78: Validation Accuracy = 77.66%\n",
      "Epoch 79: Validation Accuracy = 80.71%\n",
      "Epoch 80: Validation Accuracy = 79.70%\n",
      "Epoch 81: Validation Accuracy = 61.93%\n",
      "Epoch 82: Validation Accuracy = 77.66%\n",
      "Epoch 83: Validation Accuracy = 77.16%\n",
      "Epoch 84: Validation Accuracy = 80.71%\n",
      "Epoch 85: Validation Accuracy = 77.66%\n",
      "Epoch 86: Validation Accuracy = 79.70%\n",
      "Epoch 87: Validation Accuracy = 78.68%\n",
      "Epoch 88: Validation Accuracy = 79.19%\n",
      "Epoch 89: Validation Accuracy = 78.17%\n",
      "Epoch 90: Validation Accuracy = 77.66%\n",
      "Epoch 91: Validation Accuracy = 77.66%\n",
      "Epoch 92: Validation Accuracy = 77.66%\n",
      "Epoch 93: Validation Accuracy = 78.17%\n",
      "Epoch 94: Validation Accuracy = 78.68%\n",
      "Epoch 95: Validation Accuracy = 78.17%\n",
      "Epoch 96: Validation Accuracy = 78.17%\n",
      "Epoch 97: Validation Accuracy = 77.66%\n",
      "Epoch 98: Validation Accuracy = 76.65%\n",
      "Epoch 99: Validation Accuracy = 79.70%\n",
      "Epoch 100: Validation Accuracy = 76.65%\n",
      "\n",
      "✅ Best Model: Epoch 22 with Accuracy 84.26%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    model_path = fr\"D:\\Final EEG Graph Project\\models\\full_model\\full_model_epoch_{epoch}.pt\"\n",
    "    try:\n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                preds = (outputs > 0.5).squeeze().long()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch}: Validation Accuracy = {acc:.2f}%\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = epoch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model at epoch {epoch}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Best Model: Epoch {best_epoch} with Accuracy {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyes\\AppData\\Local\\Temp\\ipykernel_16484\\2044414851.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(r\"models/full_model/full_model_epoch_30.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Abnormal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Abnormal'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load your trained model\n",
    "model = torch.load(r\"models/full_model/full_model_epoch_30.pt\", map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Preprocessing for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2212, 1408)),  # Ensure all images are the same size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# ✅ Load and preprocess the image\n",
    "def predict_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = (output > 0.5).item()\n",
    "\n",
    "    # Show result\n",
    "    label = \"Normal\" if pred == 1 else \"Abnormal\"\n",
    "    print(f\"Prediction: {label}\")\n",
    "    return label\n",
    "\n",
    "# 🧪 Example usage\n",
    "predict_image(r\"D:\\EEG graph project\\EEG Images\\normal graphs\\P5_normal graphs_5.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560e8489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyes\\AppData\\Local\\Temp\\ipykernel_17284\\3362797156.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Result:\n",
      "⚠️ Abnormal Brain Activity Detected.\n",
      "Please consult a neurologist for further evaluation.\n",
      "(Confidence Score: 0.20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'⚠️ Abnormal Brain Activity Detected.\\nPlease consult a neurologist for further evaluation.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ✅ Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load the best model (Epoch 22 with 84.26% accuracy)\n",
    "model_path = r\"D:\\Final EEG Graph Project\\models\\full_model\\full_model_epoch_22.pt\"\n",
    "model = torch.load(model_path, map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((2212, 1408)),         # Ensure size matches training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)    # For 3 RGB channels\n",
    "])\n",
    "\n",
    "# ✅ Prediction function with friendly output\n",
    "def predict_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = (output > 0.5).item()\n",
    "        prob = output.item()\n",
    "\n",
    "    if pred == 1:\n",
    "        label = \"Normal Brain Activity Detected.\\n✅ No signs of abnormal neural patterns.\"\n",
    "    else:\n",
    "        label = (\"⚠️ Abnormal Brain Activity Detected.\\n\"\n",
    "                 \"Please consult a neurologist for further evaluation.\")\n",
    "\n",
    "    print(f\"Prediction Result:\\n{label}\\n(Confidence Score: {prob:.2f})\")\n",
    "    return label\n",
    "\n",
    "# 🧪 Example usage\n",
    "predict_image(r\"D:\\EEG graph project\\EEG Images\\abnormal graphs\\P44_abnormal graphs_0.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40b3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
